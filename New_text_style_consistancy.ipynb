{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9813d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#author: Roufa\n",
    "#input:pdf\n",
    "#extract titles, sub-titles, sections, sub-sections, headings, subheadings from the book along\n",
    "#with the page and line numbers of extracted text.\n",
    "\n",
    "#check if chapter names are in titlecase or not, if not in title case then return as inconsistent.\n",
    "#check if sections, headings are in title case or sentence case or any other case.\n",
    "#OP: return the minority case as inconsistent for headings and sections along with other case (if found.)\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/copy_assessment_tool/modules_final/')\n",
    "\n",
    "from collections import Counter\n",
    "import pdfplumber\n",
    "from fuzzywuzzy import fuzz\n",
    "import numpy as np\n",
    "\n",
    "from programmatic.TOC.extract_TOC import TOC_num\n",
    "from utils import *\n",
    "from programmatic.TOC.title_sentence_case_consistency import *\n",
    "from programmatic.json_output import return_json_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def coordinate_approach(coordinate_dict,unique_cords):\n",
    "    '''\n",
    "    This fucntion is to create the dictionary based on unique coradinate that i have and less coordinate value will be Part key to my dictionary\n",
    "    High coordinate value will be others key to my dictionary \n",
    "    '''\n",
    "    predefined_chapter_map = [\"Part\", \"Chapters\", \"sections\", \"subsections\", \"Headings\",\"others\"]\n",
    "\n",
    "    cord_map = {}\n",
    "    for groupname, cord in zip(predefined_chapter_map, unique_cords):\n",
    "        cord_map[cord] = groupname\n",
    "        \n",
    "    result_map = {}\n",
    "    for text, cord in coordinate_dict.items():\n",
    "        if cord_map[cord] not in result_map.keys():\n",
    "            result_map[cord_map[cord]] = [text]\n",
    "        else:\n",
    "            result_map[cord_map[cord]].append(text)\n",
    "        \n",
    "    return result_map\n",
    "\n",
    "\n",
    "def check_if_part_in_chapter(result_map):\n",
    "    '''\n",
    "    This function is to check if the TOC names starts with chapter palce in chapters list and if starts with parts/sections and palce there respective\n",
    "    sections and parts    \n",
    "    '''\n",
    "    preprocessed_data = {\n",
    "    'chapters': [],\n",
    "    'parts': [],\n",
    "    'sections': []\n",
    "    }\n",
    "\n",
    "    for key, value in result_map.items():\n",
    "        x = len(result_map)\n",
    "        # intendation only have two coordinate value then check for the parts and chapter in dict\n",
    "        if x <=2:\n",
    "        \n",
    "            for item in value:\n",
    "                new_item = item[:]\n",
    "                if item.lower().startswith('chapter'):\n",
    "                    preprocessed_data['chapters'].append(new_item)\n",
    "                elif item.lower().startswith('part'):\n",
    "                    preprocessed_data['parts'].append(new_item)\n",
    "                elif item.lower().startswith('sections'):\n",
    "                    preprocessed_data['parts'].append(new_item)\n",
    "                else:\n",
    "                    preprocessed_data['sections'].append(new_item)\n",
    "    # preprocessed_cleaned = {key: value for key, value in preprocessed_data.items() if value}\n",
    "    return preprocessed_data\n",
    "\n",
    "def check_number_sequencing(preprocessed_cleaned_dict):\n",
    "    '''\n",
    "    If the TOC starts with numbers like 1 or 1. then palce these numbers in chapters and if numbers has 1.1 or 1.2. then this would be in sections\n",
    "    '''\n",
    "    \n",
    "    new_dictionary = {\"chapters\":[],\"sections\":[],\"part\":[]}\n",
    "    for key,values in preprocessed_cleaned_dict.items():\n",
    "        for val in values:\n",
    "            result_split = val.split('.')\n",
    "        \n",
    "            if len(result_split) > 1:\n",
    "                if val.startswith((\"part\",\"Part\")):\n",
    "                    new_dictionary[\"part\"].append(val)\n",
    "                \n",
    "                elif len(result_split) > 1:\n",
    "                    if (result_split[0].strip().isdigit()) and (result_split[1].strip().split(' ')[0].isdigit()):\n",
    "                        new_dictionary[\"sections\"].append(val)\n",
    "                    elif (result_split[0].strip().isdigit()) and (result_split[1].strip().split(' ')[0].isalpha()):\n",
    "                        new_dictionary[\"chapters\"].append(val)\n",
    "            else:\n",
    "                space_spliting = val.split(' ')[:2]  \n",
    "                if len(space_spliting)>1:\n",
    "                    if (space_spliting[0].strip().isdigit()) and (space_spliting[1].strip().split(' ')[0].isalpha()):\n",
    "                        new_dictionary[\"chapters\"] = val\n",
    "                    else:\n",
    "                        if val.startswith((\"part\",\"Part\")):\n",
    "                            new_dictionary[\"part\"].append(val)\n",
    "                        elif val.startswith((\"chapter\",\"Chapter\")):\n",
    "                            new_dictionary[\"chapters\"].append(val)\n",
    "                        else:\n",
    "                            new_dictionary[\"sections\"].append(val)  \n",
    "\n",
    "    return new_dictionary\n",
    "            \n",
    "        \n",
    "def check_final_call(cleaned_data, unique_cords):\n",
    "    '''\n",
    "    This functions is to call the above functions likes coordinate_approach,check_if_part_in_chapter,check_number_sequencing \n",
    "    '''\n",
    "    coo = coordinate_approach(cleaned_data, unique_cords)\n",
    "    coo = {key: value for key, value in coo.items() if value}\n",
    "    '''Below condition to check if my dictionary has len of keys is 2 which means it might have chapter or part name in my TOC if \n",
    "       chapter or parts not in TOC them my toc are palcing in sections key in dictionary[cp]\n",
    "       if there is only one key which is sections key then i am checking for number sequencing fucntion\n",
    "    '''\n",
    "\n",
    "    if len(coo) <= 2:\n",
    "        cp = check_if_part_in_chapter(coo)\n",
    "        \n",
    "        coo.clear()\n",
    "        cp = {key: value for key, value in cp.items() if value}\n",
    "        \n",
    "        if (len(cp) <= 1) and (\"sections\" in cp):\n",
    "            seq = check_number_sequencing(cp)\n",
    "            cp.clear()\n",
    "            seq = {key: value for key, value in seq.items() if value}\n",
    "        else:\n",
    "            seq = {}  # Assign a default value\n",
    "    else:\n",
    "        coo = coo\n",
    "        cp = {}  # Assign a default value\n",
    "        seq = {}\n",
    "    return coo, cp, seq\n",
    "\n",
    "def chapters_page_no(pdf,num,result_set):\n",
    "    '''This fucntion is to extract the page no and line no once the chapters are matched with TOC '''\n",
    "\n",
    "    chapters = []\n",
    "    page_nos = []\n",
    "    line_nos = []\n",
    "    for page_no, page in enumerate(pdf.pages):\n",
    "\n",
    "        # after table of contents\n",
    "        if page_no > num:\n",
    "            \n",
    "            text = page.filter(filter_superscript).within_bbox((0, 0, 550, 770)).extract_text_simple()\n",
    "            textlines = text.split(\"\\n\")\n",
    "\n",
    "            for line_no , text_line in enumerate(textlines):\n",
    "                text_line = text_line\n",
    "\n",
    "                # to remove special symbols\n",
    "                text_line = re.sub(r\"[^a-zA-Z0-9 :\\n]\", \"\", text_line)\n",
    "                text_line = re.sub(r\"[:?]+$ \",\"\",text_line)\n",
    "                text_line = re.sub(r\":(?=\\s|$)\", \"\", text_line)\n",
    "                text_line = re.sub(r'(Chapter|chapter|Part|part|\\b\\d+(\\.\\d+)?\\b)',\"\",text_line)\n",
    "                text_line = re.sub(r'^\\d+', '', text_line)\n",
    "                text_line = remove_chapter_or_part_text([text_line])\n",
    "                text_line = text_line[0]\n",
    "                text_line = text_line.strip()\n",
    "                text_line = ' '.join(text_line.split())\n",
    "\n",
    "                \n",
    "                key = result_set.get(\"sections\")\n",
    "                if key is None:\n",
    "                    key = result_set.get(\"chapters\")\n",
    "                for j, res_val in enumerate(key):\n",
    "                    if fuzz.ratio(text_line, res_val) >= 93:\n",
    "\n",
    "\n",
    "                        chapters.append(res_val)\n",
    "                        page_nos.append(page_no+1)\n",
    "                        line_nos.append(line_no+1)                          \n",
    "                        \n",
    "    return chapters,page_nos,line_nos\n",
    "\n",
    "\n",
    "def append_chapters(chapters, page_nos, line_nos):\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    new_chapters,new_page_nos,new_line_nos = [],[],[]\n",
    "    for ch,pgno,lineno in zip(chapters, page_nos, line_nos):\n",
    "        val = f\"{ch}+{pgno}+{lineno}\"\n",
    "        \n",
    "        if val not in filtered:\n",
    "            filtered.append(val)\n",
    "            new_chapters.append(ch)\n",
    "            new_page_nos.append(pgno)\n",
    "            new_line_nos.append(lineno)\n",
    "    return new_chapters,new_page_nos,new_line_nos,filtered\n",
    "\n",
    "\n",
    "def bold_extraction_heading(pdf, new_chapters, new_page_nos, new_line_nos):\n",
    "    #bold content extraction for heading\n",
    "    ''' Function to extract bold content in book once match with chapters or sections inside the book '''\n",
    "    \n",
    "    dict_bold = {}\n",
    "\n",
    "    for index in range(len(new_chapters) - 1):\n",
    "        for page_no, page in enumerate(pdf.pages):\n",
    "            if page_no >= new_page_nos[index] and page_no <= new_page_nos[index + 1]:\n",
    "                bold_text = page.filter(filter_boldtext).filter(filter_superscript).extract_text()\n",
    "                bold_lines = bold_text.split(\"\\n\")\n",
    "                bold_lines = strip_word_in_line(bold_lines)\n",
    "                \n",
    "                text = page.extract_text()\n",
    "                textlines = text.split(\"\\n\")\n",
    "                \n",
    "                if bold_lines is not None:\n",
    "                    bold_lines_content = set(bold_lines) & set(textlines)\n",
    "                    \n",
    "                    for line_number, content in enumerate(textlines):\n",
    "                        if content in bold_lines_content:\n",
    "                            # Remove table of contents elements\n",
    "                            re_content = test_remove_Toc(content)\n",
    "                            \n",
    "                            # Check if the content already exists in the dictionary\n",
    "                            if re_content not in dict_bold:\n",
    "                                dict_bold[re_content] = []\n",
    "                            \n",
    "                            # Append the page_no and line_number to the dictionary if not already added\n",
    "                            if (page_no, line_number) not in dict_bold[re_content]:\n",
    "                                dict_bold[re_content].append((page_no+1, line_number+1))\n",
    "                                \n",
    "    new_cleann_dict1 = {k: v for k, v in dict_bold.items() if k}\n",
    "    return new_cleann_dict1\n",
    "\n",
    "\n",
    "#extracting italic headings\n",
    "def italic_content(pdf, new_chapters, new_page_nos):\n",
    "    #italic_content extraction for heading\n",
    "    ''' Function to extract italic_content in book once match with chapters or sections inside the book '''\n",
    "    dict_italic = {}\n",
    "    \n",
    "    for index in range(len(new_chapters) - 1):\n",
    "        for page_no, page in enumerate(pdf.pages):\n",
    "            if page_no >= new_page_nos[index] and page_no <= new_page_nos[index + 1]:\n",
    "                italic_text = page.filter(filter_Italictext).filter(filter_superscript).extract_text()\n",
    "                italic_lines = italic_text.split(\"\\n\")\n",
    "                italic_lines = strip_word_in_line(italic_lines)\n",
    "                \n",
    "                text = page.extract_text()\n",
    "                textlines = text.split(\"\\n\")\n",
    "                \n",
    "                if italic_lines is not None:\n",
    "                    sorted_len_italic = sorted(italic_lines, key=lambda line: len(italic_lines))\n",
    "                    italic_mean = np.mean([len(line) for line in italic_lines])\n",
    "                    processed_lines = [line for line in sorted_len_italic if len(line) < italic_mean-30]\n",
    "                    \n",
    "                    for line_number, content in enumerate(textlines):\n",
    "                        if content in processed_lines:\n",
    "                            # Remove table of contents elements\n",
    "                            re_content = test_remove_Toc(content)\n",
    "                            \n",
    "                            # Check if the content already exists in the dictionary\n",
    "                            if re_content not in dict_italic:\n",
    "                                dict_italic[re_content] = []\n",
    "                            \n",
    "                            # Append the page_no and line_number to the dictionary if not already added\n",
    "                            if (page_no, line_number) not in dict_italic[re_content]:\n",
    "                                dict_italic[re_content].append((page_no+1, line_number+1))\n",
    "                                \n",
    "    new_cleann_italic = {k: v for k, v in dict_italic.items() if k}\n",
    "    return new_cleann_italic\n",
    "\n",
    "#extracting headings that are not bold and italic\n",
    "def match_lines_heading(pdf, new_chapters, new_page_nos):\n",
    "    # match line which is having shorter length which considering it as headings\n",
    "    dict_lines_heading = {}\n",
    "    \n",
    "    for index in range(len(new_chapters) - 1):\n",
    "        for page_no, page in enumerate(pdf.pages):\n",
    "            if page_no >= new_page_nos[index] and page_no <= new_page_nos[index + 1]:\n",
    "                text = page.filter(filter_superscript).within_bbox((0, 0, 550, 770)).extract_text()\n",
    "                text_lines = text.split(\"\\n\")\n",
    "                shortest_lines = sorted(text_lines, key=lambda line: len(line))\n",
    "                lines_without_period = [line for line in shortest_lines if not (line.endswith(('.','?','!',\"'\",'\"')))]\n",
    "                line_mean = np.mean([len(line) for line in text_lines])\n",
    "                short_lines = [line for line in lines_without_period if len(line) < line_mean - 30]\n",
    "                processed_lines_content = set(short_lines) & set(text_lines)\n",
    "                \n",
    "                for line_number, content in enumerate(text_lines):\n",
    "                    if content in processed_lines_content:\n",
    "                        re_content = test_remove_Toc(content)\n",
    "                        \n",
    "                        # Initialize an empty list for the key if it doesn't exist\n",
    "                        if re_content not in dict_lines_heading:\n",
    "                            dict_lines_heading[re_content] = []\n",
    "                        \n",
    "                        dict_lines_heading[re_content].append((page_no+1, line_number+1))\n",
    "    dict_lines_heading_final = {k: v for k, v in dict_lines_heading.items() if k}            \n",
    "    return dict_lines_heading_final\n",
    "    \n",
    "\n",
    "def collection_cords(new_cleaned_Data):\n",
    "    # pdf plumber to extract coordinate of lines in TOC\n",
    "    cord_map = {}\n",
    "    all_cords = [cord for chapter, cord in new_cleaned_Data.items()]\n",
    "\n",
    "    # overall groups/spaces\n",
    "    unique_cords = sorted(list(set(all_cords)))\n",
    "\n",
    "    counter = Counter(all_cords)\n",
    "    predefined_chapter_map = [\"Part\", \"Chapters\", \"sections\", \"subsections\", \"Titles\",\"others\"]\n",
    "    for groupname, cord in zip(predefined_chapter_map, unique_cords):\n",
    "        cord_map[cord] = groupname\n",
    "\n",
    "    return unique_cords,cord_map\n",
    "\n",
    "\n",
    "def dict_list_contents(dict_list):\n",
    "    # This functins is to take input as list of dictionary which is return from check_final_call\n",
    "    add_diction = [] \n",
    "    for d in dict_list:\n",
    "        for d_k,d_v in d.items():\n",
    "            if d_k:\n",
    "                add_diction.append(d)\n",
    "            else:\n",
    "                print(\"No contens are extracted\",d)\n",
    "    return add_diction[0]\n",
    "\n",
    "\n",
    "def strip_dict(result_set):\n",
    "    result_set_new = {key:val for key,val in result_set.items() if  val }\n",
    "    return result_set_new\n",
    "\n",
    "def chapter_remove(data_dict):\n",
    "    if data_dict.get(\"chapters\"):\n",
    "        data_dict['chapters'] = [chapter for chapter in data_dict['chapters'] if chapter != '']\n",
    "    elif data_dict.get(\"sections\"):\n",
    "        data_dict['sections'] = [chapter for chapter in data_dict['sections'] if chapter != '']\n",
    "    else:\n",
    "        dict_find = data_dict\n",
    "    return data_dict\n",
    "\n",
    "def cleaning(result_set_data):\n",
    "    result_set_new_clean = {}\n",
    "\n",
    "    for key, values in result_set_data.items():\n",
    "        result_set_new_clean[key] = [re.sub(r'^\\.', '', value) for value in values]\n",
    "    \n",
    "    return result_set_new_clean\n",
    "\n",
    "def compare_remove(dict_2,result_set):\n",
    "    # removing the chapter and sections that are present in the TOC only take Heading\n",
    "    threshold = 75\n",
    "    new_clean = {}\n",
    "    list_complete= []\n",
    "    for val in result_set.values():\n",
    "        for app_val in val:\n",
    "            list_complete.append(app_val)\n",
    "             \n",
    "    for c_list in list_complete:\n",
    "        for new_k,new_v in dict_2.items():\n",
    "            if fuzz.ratio(new_k, c_list) < threshold:\n",
    "                \n",
    "                            new_clean[new_k] = new_v\n",
    "                            \n",
    "    return new_clean\n",
    "    \n",
    "## Extract line number of chapters and sections inside the book\n",
    "def clean_textlines(textlines):\n",
    "    new_text_lines = []\n",
    "    for line_no , text_line in enumerate(textlines):\n",
    "        text_line = text_line\n",
    "\n",
    "        # to remove special symbols\n",
    "        text_line = re.sub(r\"[^a-zA-Z0-9 :\\n]\", \"\", text_line)\n",
    "        text_line = re.sub(r\"[:?]+$ \",\"\",text_line)\n",
    "        text_line = re.sub(r\":(?=\\s|$)\", \"\", text_line)\n",
    "        text_line = re.sub(r'(Chapter|chapter|Part|part|\\b\\d+(\\.\\d+)?\\b)',\"\",text_line)\n",
    "        text_line = re.sub(r'^\\d+', '', text_line)\n",
    "        text_line = remove_chapter_or_part_text([text_line])\n",
    "        text_line = text_line[0]\n",
    "        text_line = text_line.strip()\n",
    "        text_line = ' '.join(text_line.split())\n",
    "        new_text_lines.append(text_line)\n",
    "    return new_text_lines  \n",
    "\n",
    "def get_match(text_lines, refrence_list):\n",
    "    # mactching of chapters and section inside book with TOC\n",
    "    filtered_list = []\n",
    "    for line in text_lines:\n",
    "        for line1 in refrence_list:\n",
    "            if fuzz.ratio(line, line1) >= 92:\n",
    "                filtered_list.append(line)\n",
    "    return filtered_list\n",
    "            \n",
    "\n",
    "def extract_page_line_numbers(pdf, num, result_set, keys_to_extract):\n",
    "\n",
    "    matched_data = {key: {} for key in keys_to_extract}\n",
    "\n",
    "    for page_no, page in enumerate(pdf.pages):\n",
    "        if page_no > num:\n",
    "            text = page.filter(filter_superscript).within_bbox((0, 0, 550, 770)).extract_text_simple()\n",
    "            textlines = text.split(\"\\n\")\n",
    "            textlines = clean_textlines(textlines)\n",
    "\n",
    "            for key in keys_to_extract:\n",
    "                matched_lines = get_match(textlines, result_set.get(key, []))\n",
    "                for line_no, line_content in enumerate(textlines):\n",
    "                    if line_content in matched_lines:\n",
    "                        for line_matched in matched_lines:\n",
    "                            if line_matched !='':\n",
    "                                matched_data[key][line_matched] = []\n",
    "                                matched_data[key][line_matched].append((page_no + 1, line_no + 1))\n",
    "\n",
    "    return matched_data\n",
    "\n",
    "def remove_duplicates(input_dict):\n",
    "    output_dict = {}\n",
    "\n",
    "    for category, entries in input_dict.items():\n",
    "        unique_entries = {}\n",
    "        for entry, coordinates in entries.items():\n",
    "            unique_coordinates = list(set(coordinates))\n",
    "            unique_entries[entry] = unique_coordinates\n",
    "\n",
    "        output_dict[category] = unique_entries\n",
    "\n",
    "    return output_dict\n",
    "\n",
    "\n",
    "#extract headings, subheadings, sections, subsections from all chapters in the book.\n",
    "def extract_all_titles(pdf):\n",
    "    entries_to_remove = ['Contents', 'Foreword', 'Preface','List of figures','Acknowledgment','List of contributors','List of abbreviations','Conventions','List of tables','About the Authors','References','appendix','index','Conclusions',\"Devil Is An Ass\",\"Bibliography\"]\n",
    "    entity_labels = [\"person\"]\n",
    "    keys_to_extract = [\"chapters\", \"sections\", \"part\"]\n",
    "    \n",
    "    page_content_dict, page_num = TOC_num(pdf) # same\n",
    "    cleaned_data = remove_entries(page_content_dict, entries_to_remove) # same\n",
    "    new_cleaned_data = filter_entries_by_named_entities(cleaned_data, entity_labels) # same \n",
    "    unique_cords,cord_map = collection_cords(new_cleaned_data)\n",
    "    result_map = coordinate_approach(new_cleaned_data,unique_cords)\n",
    "    pre_data = check_if_part_in_chapter(result_map)\n",
    "    preprocessed_cleaned = strip_dict(pre_data)\n",
    "    line = check_number_sequencing(preprocessed_cleaned)\n",
    "    coo,cp,seq = check_final_call(new_cleaned_data,unique_cords)\n",
    "    dict_list = [coo,cp,seq]\n",
    "    final_segregation_ch_section_part = dict_list_contents(dict_list)\n",
    "    cleaned_result = remove_numbers_Toc(final_segregation_ch_section_part)\n",
    "    new_cleaned_result = strip_dict(cleaned_result)\n",
    "    result_set=clean_roman_numbers_Toc(new_cleaned_result)\n",
    "    result_set_new = strip_dict(result_set)\n",
    "    result_set_data = chapter_remove(result_set_new)\n",
    "    result_set_new_clean = cleaning(result_set_data)\n",
    "    result_set_new_clean = strip_dict(result_set_new_clean)\n",
    "    chapters,page_nos,line_nos = chapters_page_no(pdf,page_num,result_set_new_clean)\n",
    "    new_chapters,new_page_nos,new_line_nos,filtered = append_chapters(chapters, page_nos, line_nos)\n",
    "    \n",
    "    \n",
    "    dict_bold = bold_extraction_heading(pdf,new_chapters,new_page_nos,new_line_nos)\n",
    "    new_clean_bold = compare_remove(dict_bold,result_set_new_clean)\n",
    "    dict_italic = italic_content(pdf,new_chapters,new_page_nos)\n",
    "    new_clean_italic = compare_remove(dict_italic,result_set_new_clean)\n",
    "    \n",
    "    \n",
    "    append_two_dicts = new_clean_bold.copy()  # Make a copy of dict_1\n",
    "\n",
    "    # Update dict_1 with the contents of dict_2\n",
    "    append_two_dicts.update(new_clean_italic)\n",
    "    \n",
    "    if (not new_clean_bold) and (not new_clean_italic) and (not append_two_dicts):\n",
    "        dict_lines = match_lines_heading(pdf,new_chapters,new_page_nos)\n",
    "        new_clean_lines = compare_remove(dict_lines,result_set_new_clean)\n",
    "        extract_page_line = extract_page_line_numbers(pdf, page_num, result_set_new_clean, keys_to_extract)\n",
    "        keys_list = list(extract_page_line.keys())\n",
    "        if new_clean_lines and (\"sections\" in  keys_list):\n",
    "            combined_dict = {\n",
    "                'heading': new_clean_lines,\n",
    "                'sections': extract_page_line['sections']\n",
    "                }\n",
    "        elif new_clean_lines and (\"chapters\" in  keys_list):\n",
    "            combined_dict = {\n",
    "                'heading': new_clean_lines,\n",
    "                'chapters': extract_page_line['chapters'],\n",
    "                \n",
    "                }\n",
    "        else:\n",
    "            if extract_page_line:\n",
    "                combined_dict = {\n",
    "                'heading': append_two_dicts,\n",
    "                'chapters': extract_page_line['chapters'],\n",
    "                'sections': extract_page_line['sections']\n",
    "                }\n",
    "            else:\n",
    "                combined_dict={}\n",
    "    extract_page_line = extract_page_line_numbers(pdf, page_num, result_set_new_clean, keys_to_extract)\n",
    "    keys_list = list(extract_page_line.keys())\n",
    "    if len(extract_page_line) ==2:\n",
    "        combined_dict = {\n",
    "        'heading': append_two_dicts,\n",
    "        'chapters': extract_page_line['chapters'],\n",
    "        'sections': extract_page_line['sections']\n",
    "        }\n",
    "    elif (len(extract_page_line)<=1) and (\"chapters\" in  keys_list):\n",
    "             combined_dict = {\n",
    "        'heading': append_two_dicts,\n",
    "        'chapters': extract_page_line['chapters'],\n",
    "        \n",
    "        }\n",
    "    elif (len(extract_page_line)<=1) and (\"sections\" in  keys_list):\n",
    "         combined_dict = {\n",
    "        'heading': append_two_dicts,\n",
    "        'sections': extract_page_line['sections']\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        if extract_page_line:\n",
    "            combined_dict = {\n",
    "            'heading': append_two_dicts,\n",
    "            'chapters': extract_page_line['chapters'],\n",
    "            'sections': extract_page_line['sections']\n",
    "            }\n",
    "        else:\n",
    "            combined_dict={}\n",
    "            \n",
    "    combined_dict = remove_duplicates(combined_dict)\n",
    "\n",
    "    return combined_dict\n",
    "    \n",
    "#checking style consistency in overall book\n",
    "def check_text_title_sent_case_inconsistency(pdf):\n",
    "    inconsistent_text_count = 0\n",
    "\n",
    "    combined_dict = extract_all_titles(pdf)\n",
    "    chapters, sections, headings, _ = process_text_data(combined_dict)\n",
    "    count_non_title_case_chapters, inconsistent_title_case_chapters, inconsistent_sections, inconsistent_headings = get_inconsisent_details(chapters, sections, headings)\n",
    "\n",
    "    inconsistent_sec_counts = 0\n",
    "    inconsistent_sec_details = [{}]\n",
    "    if inconsistent_sections:\n",
    "        inconsistent_sec = [my_dict[key]  for my_dict in inconsistent_sections for key in my_dict if key.startswith('count')]\n",
    "        inconsistent_sec_counts = sum(inconsistent_sec)\n",
    "        #inconsistent_sec_details = [my_dict[key] for my_dict in inconsistent_sections for key in my_dict if key.endswith('details')]\n",
    "        for my_dict in inconsistent_sections:\n",
    "            for key, value in my_dict.items():\n",
    "                if key.endswith('details'):\n",
    "                    inconsistent_sec_details[0].update(value)\n",
    "\n",
    "\n",
    "    inconsistent_heading_counts = 0\n",
    "    inconsistent_heading_details = [{}]\n",
    "    if inconsistent_headings:\n",
    "        inconsistent_head = [my_dict[key]  for my_dict in inconsistent_headings for key in my_dict if key.startswith('count')]\n",
    "        inconsistent_heading_counts = sum(inconsistent_head)\n",
    "        #inconsistent_heading_details = [my_dict[key] for my_dict in inconsistent_headings for key in my_dict if key.endswith('details')]\n",
    "        for my_dict in inconsistent_headings:\n",
    "            for key, value in my_dict.items():\n",
    "                if key.endswith('details'):\n",
    "                    inconsistent_sec_details[0].update(value)\n",
    "\n",
    "    inconsistent_text_count = count_non_title_case_chapters + inconsistent_sec_counts + inconsistent_heading_counts\n",
    "    \n",
    "    return inconsistent_text_count, inconsistent_title_case_chapters, inconsistent_sec_details, inconsistent_heading_details\n",
    "\n",
    "    \n",
    "\n",
    "def get_inconsistent_text_bboxes(file_path):\n",
    "    pdf = pdfplumber.open(file_path)\n",
    "    inconsistent_text_count, inconsistent_title_case_chapters, inconsistent_sec_details, inconsistent_heading_details = check_text_title_sent_case_inconsistency(pdf)\n",
    "\n",
    "    inconsistent_heading_details += [inconsistent_sec_details[0], inconsistent_title_case_chapters]\n",
    "    text_json,length = final_consistent_text_json(pdf,inconsistent_heading_details)\n",
    "    \n",
    "    # print(text_json)\n",
    "    final_result_json = return_json_result(text_json)\n",
    "    return text_json,final_result_json\n",
    "\n",
    "\n",
    "\n",
    "# file_path = '/data/copy_assessment_tool/modules/data/15032-5196-FullBook.pdf'\n",
    "file_path ='/data/copy_assessment_tool/modules/data/15031-4988-FullBook.pdf'\n",
    "\n",
    "text_json,final_result_json = get_inconsistent_text_bboxes(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
