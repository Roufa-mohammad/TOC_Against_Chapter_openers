{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31682b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@Author: Roufa\n",
    "\n",
    "'''\n",
    "check if toc are in sentence case or title case.\n",
    "ip:pdf\n",
    "op: check whether TOC are consistent in style or not. we will go with dominant style as conistent style\n",
    "and return non-domiant style bboxes.\n",
    "'''\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/copy_assessment_tool/modules_final/')\n",
    "\n",
    "import pdfplumber\n",
    "from collections import Counter\n",
    "\n",
    "from programmatic.TOC.extract_TOC import TOC_num\n",
    "from utils import *\n",
    "from title_sentence_case_consistency import *\n",
    "from programmatic.json_output import return_json_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_page_content(page_content_dict):\n",
    "    new_dict = {}\n",
    "\n",
    "    for key, value in page_content_dict.items():\n",
    "        parts = key.split('~')\n",
    "        chapter_title = parts[2]\n",
    "        page_number = float(value)\n",
    "        page_and_line = f\"{parts[0]},{parts[1]}\"\n",
    "        new_dict[chapter_title] = (page_number, page_and_line)\n",
    "\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def coordinate_approach_new(coordinate_dict, unique_cords):\n",
    "    '''This create the dicitonary with there respective keys based on coordinate'''\n",
    "    predefined_chapter_map = [\"Part\", \"Chapters\", \"sections\", \"subsections\", \"Headings\", \"others\"]\n",
    "\n",
    "    cord_map = {}\n",
    "    for groupname, cord in zip(predefined_chapter_map, unique_cords):\n",
    "        cord_map[cord] = groupname\n",
    "\n",
    "    result_map = {'Part': {}, 'Chapters': {},\"sections\":{}}\n",
    "    \n",
    "    for text, cord in coordinate_dict.items():\n",
    "        group_name = cord_map[cord[0]]\n",
    "        if group_name == \"Part\":\n",
    "            result_map['Part'][text] = [(int(cord[1].split(',')[0]), int(cord[1].split(',')[1]))]\n",
    "        elif group_name == \"Chapters\":\n",
    "            result_map['Chapters'][text] = [(int(cord[1].split(',')[0]), int(cord[1].split(',')[1]))]\n",
    "        elif group_name == \"sections\":\n",
    "            result_map['sections'][text] = [(int(cord[1].split(',')[0]), int(cord[1].split(',')[1]))]\n",
    "        \n",
    "    result_map = {key : val for key,val in result_map.items() if val}\n",
    "\n",
    "\n",
    "    return result_map\n",
    "\n",
    "def process_chapters(new_cleaned_data):\n",
    "    '''This will extract coordinate or indentation values of each line of text in Toc'''\n",
    "    predefined_chapter_map = [\"Part\", \"Chapters\", \"sections\", \"subsections\", \"Titles\",\"others\"]\n",
    "    all_cords = [cord[0] for cord in new_cleaned_data.values()]\n",
    "    unique_cords = sorted(list(set(all_cords)))\n",
    "    counter = Counter(all_cords)\n",
    "\n",
    "    cord_map = {}\n",
    "    for groupname, cord in zip(predefined_chapter_map, unique_cords):\n",
    "        cord_map[cord] = groupname\n",
    "\n",
    "    return cord_map, unique_cords\n",
    "\n",
    "def check_if_part_in_chapter_new(result_map):\n",
    "    \n",
    "    ''' This function is used to segregate the toc based on if it starts with chapter,section and part it \n",
    "    will place in chapter  respective keys in dictionary '''\n",
    "    preprocessed_data = {\n",
    "    'chapters': {},\n",
    "    'parts': {},\n",
    "    'sections': {}\n",
    "    }\n",
    "\n",
    "    for key, value in result_map.items():    \n",
    "        x = len(result_map)\n",
    "  \n",
    "        # intendation only have two coordinate value then check for the parts and chapter in dict\n",
    "        if x <=2:\n",
    "        \n",
    "            for item_k,item_val in value.items():\n",
    "                new_item = item_k[:]\n",
    "                if item_k.lower().startswith(('chapter',\"Chapter\")):\n",
    "                    preprocessed_data['chapters'][new_item] = item_val\n",
    "                elif item_k.lower().startswith('part'):\n",
    "                    preprocessed_data['parts'][new_item] = item_val\n",
    "                elif item_k.lower().startswith('sections'):\n",
    "                    preprocessed_data['parts'][new_item] = item_val\n",
    "                else:\n",
    "                    preprocessed_data['sections'][new_item] = item_val\n",
    "    # preprocessed_cleaned = {key: value for key, value in preprocessed_data.items() if value}\n",
    "    preprocessed_data = {key : val for key,val in preprocessed_data.items() if val}\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "def check_number_sequencing_new(preprocessed_cleaned_dict):\n",
    "    '''this funciton used to check for number sequence of the chapters or sections '''\n",
    "    \n",
    "    new_dictionary = {\"chapters\":{},\"sections\":{},\"part\":{}}\n",
    "    for key,values in preprocessed_cleaned_dict.items():\n",
    "        for val_k,val_v in values.items():\n",
    "            result_split = val_k.split('.')\n",
    "        \n",
    "            if len(result_split) > 1:\n",
    "                if val_k.startswith((\"part\",\"Part\")):\n",
    "                    new_dictionary[\"part\"][val_k] = val_v\n",
    "                \n",
    "                elif len(result_split) > 1:\n",
    "                    if (result_split[0].strip().isdigit()) and (result_split[1].strip().split(' ')[0].isdigit()):\n",
    "                        new_dictionary[\"sections\"][val_k] = val_v\n",
    "                    elif (result_split[0].strip().isdigit()) and (result_split[1].strip().split(' ')[0].isalpha()):\n",
    "                        new_dictionary[\"chapters\"][val_k] = val_v\n",
    "            else:\n",
    "                space_spliting = val_k.split(' ')[:2]  \n",
    "                if len(space_spliting)>1:\n",
    "                    if (space_spliting[0].strip().isdigit()) and (space_spliting[1].strip().split(' ')[0].isalpha()):\n",
    "                        new_dictionary[\"chapters\"][val_k] = val_v\n",
    "                    else:\n",
    "                        if val_k.startswith((\"part\",\"Part\")):\n",
    "                            new_dictionary[\"part\"][val_k] = val_v\n",
    "                        elif val_k.startswith((\"chapter\",\"Chapter\")):\n",
    "                            new_dictionary[\"chapters\"][val_k] = val_v\n",
    "                        else:\n",
    "                            new_dictionary[\"sections\"][val_k] = val_v\n",
    "              \n",
    "    new_dictionary= {key : val for key,val in new_dictionary.items() if val}\n",
    "    return new_dictionary\n",
    "\n",
    "\n",
    "def check_final_call_new(cleaned_data, unique_cords):\n",
    "    coo = coordinate_approach_new(cleaned_data, unique_cords)\n",
    "    coo = {key: value for key, value in coo.items() if value}\n",
    "    \n",
    "    if len(coo) <= 2:\n",
    "        cp = check_if_part_in_chapter_new(coo)\n",
    "        \n",
    "        coo.clear()\n",
    "        cp = {key: value for key, value in cp.items() if value}\n",
    "        \n",
    "        if (len(cp) <= 1) and (\"sections\" in cp):\n",
    "            seq = check_number_sequencing_new(cp)\n",
    "            cp.clear()\n",
    "            seq = {key: value for key, value in seq.items() if value}\n",
    "        else:\n",
    "            seq = {}  # Assign a default value\n",
    "    else:\n",
    "        coo = coo\n",
    "        cp = {}  # Assign a default value\n",
    "        seq = {}\n",
    "    return coo, cp, seq\n",
    "\n",
    "\n",
    "def dict_list_contents(dict_list):\n",
    "    # This functins is to take input as list of dictionary which is return from check_final_call\n",
    "    add_diction = [] \n",
    "    for d in dict_list:\n",
    "        for d_k,d_v in d.items():\n",
    "            if d_k:\n",
    "                add_diction.append(d)\n",
    "            else:\n",
    "                print(\"No contens are extracted\",d)\n",
    "    return add_diction[0]\n",
    "\n",
    "\n",
    "def final_chapter_call(pdf):\n",
    "\n",
    "    entity_labels = [\"person\"]\n",
    "    entries_to_remove = ['Contents', 'Foreword', 'Preface', 'List of figures', 'Acknowledgment', 'List of contributors', 'List of abbreviations', 'Conventions', 'List of tables', 'About the Authors', 'References', 'appendix', 'index', 'Conclusions', \"Devil Is An Ass\", \"Bibliography\"]\n",
    "\n",
    "    page_content_dict, _ = TOC_num(pdf) \n",
    "    new_dict_Toc= process_page_content(page_content_dict)\n",
    "    filtered_page_content=remove_entries(new_dict_Toc,entries_to_remove) \n",
    "    new_cleaned_data = filter_entries_by_named_entities(filtered_page_content,entity_labels)\n",
    "    cord_map, unique_cords = process_chapters(new_cleaned_data)\n",
    "    coo, cp, seq= check_final_call_new(new_cleaned_data, unique_cords)\n",
    "    dict_list= [coo, cp, seq]\n",
    "    final_dict_toc= dict_list_contents(dict_list)\n",
    "    cc_Clean = remove_numbers_Toc_new(final_dict_toc)\n",
    "    return cc_Clean\n",
    "\n",
    "\n",
    "def check_TOC_title_sent_case_inconsistency(pdf):\n",
    "    inconsistent_TOC_count = 0\n",
    "\n",
    "    cc_Clean = final_chapter_call(pdf)\n",
    "    chapters, sections, _, parts = process_text_data(cc_Clean)\n",
    "\n",
    "    print(chapters)\n",
    "    print()\n",
    "    print(sections)\n",
    "    print()\n",
    "    print(parts)\n",
    "    print()\n",
    "    count_non_title_case_chapters, non_title_case_chapters, count_non_title_case_parts, non_title_case_parts, inconsistent_sections = get_inconsisent_TOC(chapters, parts, sections)\n",
    "\n",
    "    #get inconsistent section counts and details\n",
    "    inconsistent_sec_counts = 0\n",
    "    inconsistent_sec_details = [{}]\n",
    "    if inconsistent_sections:\n",
    "        inconsistent_sec = [my_dict[key]  for my_dict in inconsistent_sections for key in my_dict if key.startswith('count')]\n",
    "        inconsistent_sec_counts = sum(inconsistent_sec)\n",
    "\n",
    "        #inconsistent_sec_details = [my_dict[key] for my_dict in inconsistent_sections for key in my_dict if key.endswith('details')]\n",
    "        for my_dict in inconsistent_sections:\n",
    "            for key, value in my_dict.items():\n",
    "                if key.endswith('details'):\n",
    "                    inconsistent_sec_details[0].update(value)\n",
    "\n",
    "    inconsistent_TOC_count = count_non_title_case_chapters + count_non_title_case_parts + inconsistent_sec_counts\n",
    "    return inconsistent_TOC_count, non_title_case_chapters, non_title_case_parts, inconsistent_sec_details\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_inconsistent_TOC_bboxes(file_path):\n",
    "    results = []\n",
    "    pdf = pdfplumber.open(file_path)\n",
    "    inconsistent_TOC_count, non_title_case_chapters, non_title_case_parts, inconsistent_sec_details = check_TOC_title_sent_case_inconsistency(pdf)\n",
    "\n",
    "    inconsistent_sec_details += [non_title_case_chapters, non_title_case_parts]\n",
    "    results,length = final_consistent_text_json(pdf,inconsistent_sec_details)\n",
    "    \n",
    "    inconsistent_TOC_bboxes = return_json_result(results)\n",
    "    return inconsistent_TOC_count, inconsistent_TOC_bboxes\n",
    "\n",
    "# file_path = '/data/copy_assessment_tool/modules/data/15032-5196-FullBook.pdf'\n",
    "file_path ='/data/copy_assessment_tool/modules/data/15031-4988-FullBook.pdf'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_inconsistent_TOC_bboxes(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
